{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Again\n",
    "\n",
    "Preprocess UVMMC nifti data.  Store it on disk.  This saves a lot of time reading and preprocessing the images and speeds up training (a lot).  Preprocessing resizes every voxel to be 1mm^3.  Every scan is resized accordingly.  Every voxel is clipped to -1000 to 1000 hounsfield units and then normalized to lie between 0 and 1.  Scans are much smaller and load much faster from disk.  Metadata about the scans, like their new voxel dimensions or the path of the preprocessed image, is stored in a pickle somewhere that you can read and write.  This metadata is used for data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, Conv3D, \n",
    "                          UpSampling3D, BatchNormalization)\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import util\n",
    "importlib.reload(util)\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "PATCH_SHAPE = (32, 32, 32)\n",
    "\n",
    "MODEL_NAME = 'model_01'\n",
    "\n",
    "DATA_DIR = Path('/data2').expanduser()\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "FRACTURE_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_fractures'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_LOG_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_LOG_DIR, TMP_DIR, PP_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Read in the original images.\n",
    "2. Resample the image s.t the voxel size is 1mm x 1mm x 1mm.\n",
    "3. Clip houndsfield unit values and normalize them to be between 0 and 1. (like Julian de Wit recommends for Kaggle)\n",
    "\n",
    "Some processing code is from https://github.com/juliandewit/kaggle_ndsb2017/blob/master/step1_preprocess_luna16.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_image(path):\n",
    "    # read the image from the filesystem\n",
    "    img = nib.load(path).get_data()\n",
    "    return img\n",
    "   \n",
    "    \n",
    "def get_preprocessed_image(path):\n",
    "    return np.load(path)\n",
    "    \n",
    "\n",
    "def resample_image(image, spacing, new_spacing):\n",
    "    '''\n",
    "    image: a 3d volume\n",
    "    spacing: the size of a voxel in some units.  E.g. [0.3, 0.3, 0.9]\n",
    "    new_spacing: the size of a voxel after resampling, in some units.  E.g. [1.0, 1.0, 1.0]\n",
    "    \n",
    "    returns: resampled image and new spacing adjusted because images have integer dimensions.\n",
    "    '''\n",
    "    # calculate resize factor required to change image to new shape\n",
    "    spacing = np.array(spacing)\n",
    "    new_spacing = np.array(new_spacing)\n",
    "    spacing_resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * spacing_resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    \n",
    "    # adjusted spacing to account for integer dimensions of resized image.\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    new_image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n",
    "    return new_image, new_spacing\n",
    "\n",
    "\n",
    "def normalize_nifti_image(image):\n",
    "    '''\n",
    "    Normalize voxel units by clipping them to lie between -1000 and 1000 hounsfield units \n",
    "    and then scale number to between 0 and 1.\n",
    "    '''\n",
    "    MIN_BOUND = -1000.0 # Air: -1000, Water: 0 hounsfield units.\n",
    "    MAX_BOUND = 1000.0 # Bone: 200, 700, 3000.  https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image > 1] = 1.\n",
    "    image[image < 0] = 0.\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_preprocessed_image_path(scan_id, preprocessed_dir):\n",
    "    return str(Path(preprocessed_dir, f'{scan_id}.npy'))\n",
    "\n",
    "\n",
    "def preprocess_nifti_scans(normals_dir, fractures_dir, dest_dir, \n",
    "                             metadata_path, delete_existing=False):\n",
    "    \n",
    "    if delete_existing and dest_dir.isdir():\n",
    "        print('Removing existing dest dir:', dest_dir)\n",
    "        shutil.rmtree(dest_dir)\n",
    "    if not dest_dir.exists():\n",
    "        print('Making preprocessed images destination:', dest_dir)\n",
    "        dest_dir.mkdir(parents=True)\n",
    "    \n",
    "    # get all scan infos\n",
    "    normal_infos = get_data_infos(get_nifti_files(normals_dir))\n",
    "    fracture_infos = get_data_infos(get_nifti_files(fractures_dir))\n",
    "\n",
    "    # add class label\n",
    "    normal_infos['class'] = 'normal'\n",
    "    fracture_infos['class'] = 'fracture'\n",
    "    \n",
    "    # process and save each image.\n",
    "    infos = pd.concat([normal_infos, fracture_infos]).reset_index(drop=True) # index from 0:len(infos)\n",
    "\n",
    "    for i in range(len(infos)):\n",
    "        print('image index:', i)\n",
    "        info = infos.loc[i, :]\n",
    "        img_path = str(info['path'])\n",
    "        print('image path:', img_path)\n",
    "        scan_id = info['id']\n",
    "        print('image id:', scan_id)\n",
    "        img = get_image(img_path)\n",
    "        print('image shape:', img.shape)\n",
    "        \n",
    "        # Standardize voxel size to 1mm^3 to reduce image size.\n",
    "        spacing = (info['pixdim0'], info['pixdim1'], info['pixdim2'])\n",
    "        target_spacing = (1.0, 1.0, 1.0)\n",
    "        print('image spacing:', spacing)\n",
    "        print('new spacing:', target_spacing)\n",
    "        resampled_img, resampled_spacing = resample_image(img, spacing, target_spacing)\n",
    "        print('resampled image spacing:', resampled_spacing)\n",
    "        print('resampled image shape:', resampled_img.shape)\n",
    "        \n",
    "        normalized_img = normalize_image(resampled_img)\n",
    "        print('Normalized image shape:', normalized_img.shape)\n",
    "        \n",
    "        # save processed image\n",
    "        path = get_preprocessed_image_path(scan_id, dest_dir)\n",
    "        print(f'Saving preprocessed image to {path}.')\n",
    "        np.save(path, normalized_img)\n",
    "        \n",
    "        # track image metadata\n",
    "        infos.loc[i, 'pp_path'] = str(path)\n",
    "        # voxel dimensions\n",
    "        infos.loc[i, 'pp_pixdim0'] = resampled_spacing[0] \n",
    "        infos.loc[i, 'pp_pixdim1'] = resampled_spacing[1] \n",
    "        infos.loc[i, 'pp_pixdim2'] = resampled_spacing[2] \n",
    "        # image dimensions\n",
    "        infos.loc[i, 'pp_dim0'] = resampled_img.shape[0]\n",
    "        infos.loc[i, 'pp_dim1'] = resampled_img.shape[1]\n",
    "        infos.loc[i, 'pp_dim2'] = resampled_img.shape[2]\n",
    "        \n",
    "\n",
    "    # save metadata\n",
    "    write_preprocessed_nifti_metadata(infos, path=metadata_path)\n",
    "    return infos\n",
    "    \n",
    "        \n",
    "def write_preprocessed_nifti_metadata(infos, path):\n",
    "    print('saving preproccessed metadata to', path)\n",
    "    with open(path, 'wb') as fh:\n",
    "        fh.write(pickle.dumps(infos))\n",
    "    \n",
    "    \n",
    "def read_preprocessed_nifti_metadata(path):\n",
    "    print('reading preproccessed metadata from', path)\n",
    "    with open(path, 'rb') as fh:\n",
    "        infos = pickle.loads(fh.read())\n",
    "    \n",
    "    return infos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Images and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to preprocess images\n",
    "# infos = preprocess_nifti_scans(NORMAL_SCANS_DIR, FRACTURE_SCANS_DIR, dest_dir=PP_IMG_DIR, metadata_path=PP_MD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Validating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test getting a raw image\n",
    "data_infos = infos\n",
    "img_info = data_infos.iloc[0]\n",
    "img = get_image(img_info['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_crop(img, axis=2, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the resampled image has more or less the shape we expect it to have after resizing the voxels.\n",
    "\n",
    "img_spacing = (img_info['pixdim0'], img_info['pixdim1'], img_info['pixdim2'])\n",
    "print('Shape and spacing before resampleing\\t', img.shape, img_spacing)\n",
    "target_img_spacing = (1., 1., 1.)\n",
    "print('Target spacing:', target_img_spacing)\n",
    "resampled_img, resampled_spacing = resample_image(img, img_spacing, target_img_spacing)\n",
    "print (\"Shape after resampling\\t\", resampled_img.shape, resampled_spacing)\n",
    "animate_crop(resampled_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading metadata, which contains the preprocessed image spacings and file paths\n",
    "infos = read_preprocessed_metadata(PP_MD_PATH)\n",
    "pp_spacings = list(zip(infos['pp_pixdim0'], infos['pp_pixdim1'], infos['pp_pixdim2']))\n",
    "pp_paths = list(infos['pp_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_spacings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that preprocessed images look reasonable when visualized\n",
    "for i in range(3):\n",
    "    img = get_preprocessed_image(infos.loc[i, 'pp_path'])\n",
    "    scan_id = infos.loc[i, 'id']\n",
    "    print(f'image {i} scan id {scan_id} shape {img.shape}')\n",
    "    display(animate_crop(img))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
