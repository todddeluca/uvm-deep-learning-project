{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 16 Straight Net\n",
    "\n",
    "This is the low baseline control.  A convolutional network that starts from the input and ends at the output.  No downsampling!  The idea is to explore the limits of memory (how many layers can the network be) and field of view (fov) using full-sized volumes and large kernel convolutions.  \n",
    "\n",
    "Things I've tried:\n",
    "\n",
    "- With and without residual blocks (no evidence either way).\n",
    "- Channels: Increasing the number of channels from 1 to 4 (more channels seems better, but increases memory)\n",
    "- Blocks: Increasing the number of blocks from 1 to 16 (more layers seems better.)\n",
    "- Kernel Size: Using 5x5x5 convolutions to increase field of view, though the f.o.v. of a u-net is much larger.\n",
    "- Patch Size: Train on patch sizes from 32x32x32 to 128x128x128.\n",
    "- Memory Tuning: Test predicting on full size validation to tune \"volume\" of network activations\n",
    "- Using GaussianNoise on the input.  (Does not seem to help at 0.001)\n",
    "- Add initial Conv before first residual.  (No evidence either way)\n",
    "- Increasing Epochs (No evidence.)\n",
    "- Increase kernel size to 7x7x7.\n",
    "\n",
    "\n",
    "To try:\n",
    "\n",
    "- making sure my loss functions work\n",
    "- using dilated convolutions\n",
    "- Using Dropout (try 0.1)\n",
    "- A shallow u-net: Pooling once and taking advantage of the smaller volume to increase channels and layers.  This would lead to a greatly increased fov.\n",
    "- Explore relationship between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D,\n",
    "                          Conv3D, UpSampling3D, BatchNormalization, Concatenate, Add,\n",
    "                          GaussianNoise, Dropout\n",
    "                         )\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import SimpleITK # xvertseg MetaImage files\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import util\n",
    "import preprocessing\n",
    "import datagen\n",
    "import modelutil\n",
    "import xvertseg\n",
    "import augmentation\n",
    "import metrics\n",
    "\n",
    "MODEL_NAME = 'model_16'\n",
    "\n",
    "DATA_DIR = Path('/data2').expanduser()\n",
    "# DATA_DIR = Path('~/data/2018').expanduser()\n",
    "# UVMMC\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "# xVertSeg\n",
    "XVERTSEG_DIR = DATA_DIR / 'xVertSeg.v1'\n",
    "PP_XVERTSEG_DIR = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed' # preprocessed scans dir\n",
    "PP_XVERTSEG_MD_PATH = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_DIR, TMP_DIR, PP_MD_PATH.parent, PP_XVERTSEG_DIR, PP_XVERTSEG_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# I love u autoreload!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25 # random seed for dataset shuffling and splitting.\n",
    "VALIDATION_SPLIT = 0.2 # 3 samples for validation\n",
    "TEST_SPLIT = 0.134 # 2 samples for test\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_BATCHES = 20\n",
    "MAX_QUEUE_SIZE = 20\n",
    "EPOCHS = 30\n",
    "NUM_SAMPLES = 1 # number of samples taken per image during training.  Increase epoch size.\n",
    "\n",
    "# PATCH_SHAPE = (32, 32, 32)\n",
    "# PATCH_SHAPE = (64, 64, 64) # Used to crop images for training (data augmentation, memory, speed)\n",
    "PATCH_SHAPE = (128, 128, 128) # Big.  Good for visualization.\n",
    "# PATCH_SHAPE = None # Full sized images\n",
    "\n",
    "# INPUT_SHAPE = (PATCH_SHAPE + (1,)) # Model input shape adds channel dimension, but not examples dim.\n",
    "INPUT_SHAPE = (None, None, None, 1) # Accept variable size volumes/images.\n",
    "\n",
    "BINARY_MASK_THRESH = 0.5 # > threshold = 1. <= thresh = 0.\n",
    "\n",
    "TRANSPOSE = False\n",
    "FLIP = 0.5\n",
    "GRAY_STD = 0.01\n",
    "\n",
    "# Visualize model using the first set of hyperparams\n",
    "# KERNEL_SIZE = (5, 5, 5)\n",
    "KERNEL_SIZE = (7, 7, 7)\n",
    "# n_a = 2, n_r = 8.  NaN loss.  Why?\n",
    "N_A = 4 # number of channels # 4 and 4 works for full sized testing, I think, memory-wise.\n",
    "N_B = 4 # number of blocks of residual blocks.\n",
    "N_R = 4 # number of repeated layers/blocks.  33 pixel field of view after 8 5x convolutions.\n",
    "DROPOUT = None # 0.1\n",
    "NOISE = None # 0.0001\n",
    "\n",
    "W0 = 1 # binary cross entropy weight for class 0\n",
    "W1 = 100 # weight informed by the 1-to-0 ratio in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_func = lambda: xvertseg.read_xvertseg_metadata(PP_XVERTSEG_MD_PATH)\n",
    "train_gen, val_gen, test_gen = xvertseg.get_xvertseg_datagens(\n",
    "    infos_func, seed=SEED, validation_split=VALIDATION_SPLIT, test_split=TEST_SPLIT)\n",
    "\n",
    "train_gen.config(batch_size=BATCH_SIZE, length=N_BATCHES, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "                 transpose=TRANSPOSE, gray_std=GRAY_STD, num_samples=NUM_SAMPLES).reindex()\n",
    "val_gen.config(batch_size=BATCH_SIZE, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "               transpose=TRANSPOSE, gray_std=GRAY_STD).reindex()\n",
    "# val_gen.config(batch_size=1).reindex() # Test full image\n",
    "test_gen.config(batch_size=1).reindex() # Evaluate using full image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, n_a=4, n_b=4, n_r=4, dropout=None, noise=None, loss='binary_crossentropy', metrics=[],\n",
    "                kernel_size=3):\n",
    "    '''\n",
    "    3D convolutional straight convolutional segmenter.\n",
    "    \n",
    "    dropout: proportion of activation of input to drop out. 0.0 to 1.0\n",
    "    noise: std dev of noise added to input activation.\n",
    "    w0: > 0.0.  A weight for this class in the binary cross entropy loss.\n",
    "    w1: > 0.0.  A weight for this class in the binary cross entropy loss.\n",
    "\n",
    "    returns: Keras model\n",
    "    '''\n",
    "\n",
    "    x_input = Input(shape=input_shape)\n",
    "    x = x_input\n",
    "    \n",
    "    # noise regularization\n",
    "    if noise: \n",
    "        x = GaussianNoise(stddev=noise)(x)\n",
    "\n",
    "    x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Dropout followed by n_r convolutions.\n",
    "    for i in range(n_b):\n",
    "        if dropout is not None:\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "            \n",
    "        for j in range(n_r):\n",
    "            x_initial = x\n",
    "            # x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "            x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "            x = Add()([x_initial, x])  \n",
    "        \n",
    "\n",
    "    y = Conv3D(1, kernel_size=(1, 1, 1), activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'] + metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_binary_crossentropy_loss = metrics.weighted_binary_crossentropy_loss_func(w0=W0, w1=W1)\n",
    "dice_coefficient_loss = metrics.dice_coefficient_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape=INPUT_SHAPE, n_a=N_A, n_r=N_R, \n",
    "                    dropout=DROPOUT, noise=NOISE, \n",
    "#                     loss='binary_crossentropy',\n",
    "                    loss=dice_coefficient_loss,\n",
    "                    metrics=[metrics.dice_coefficient, metrics.binary_dice_coefficient],\n",
    "                    kernel_size=KERNEL_SIZE)\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [modelutil.get_tensorboard_callback(TENSORBOARD_DIR, MODEL_NAME),\n",
    "             modelutil.get_logger_callback(LOG_DIR, MODEL_NAME),\n",
    "             modelutil.get_checkpoint_callback(MODELS_DIR, MODEL_NAME),\n",
    "            ]\n",
    "# datagen shuffles every epoch\n",
    "history = model.fit_generator(train_gen, epochs=EPOCHS, validation_data=val_gen, \n",
    "                              callbacks=callbacks, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                              use_multiprocessing=False, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metrics from the log file\n",
    "# get latest log path\n",
    "log_path = sorted(LOG_DIR.glob(f'{MODEL_NAME}*_log.csv'))[-1]\n",
    "print(log_path)\n",
    "log_data = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([log_data[::10], log_data[-1:]]) # every 10th metric and the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_a=4, n_b=4, n_r=4, kernel_size=(7, 7, 7), noise=None, dropout=None\n",
    "Increase kernel size.  More params!  More fov!  Very slow to train!\n",
    "Results: Terrible performance.  All zeros all the time.\n",
    "Total params: 89,253\n",
    "Trainable params: 89,253\n",
    "\n",
    "```\n",
    "\tepoch\tacc\tdice_coefficient\tloss\tval_acc\tval_dice_coefficient\tval_loss\n",
    "0\t0\t0.957572\t0.301465\t-0.301465\t0.987629\t0.666671\t-0.666671\n",
    "10\t10\t0.989230\t0.701384\t-0.701384\t0.973136\t0.666669\t-0.666669\n",
    "20\t20\t0.991169\t0.750005\t-0.750005\t0.972196\t0.333375\t-0.333375\n",
    "29\t29\t0.989198\t0.552355\t-0.552355\t0.958105\t0.000826\t-0.000826\n",
    "```\n",
    "\n",
    "\n",
    "n_r = 16, n_a = 4.  Noise=0.0001.  \n",
    "More Epochs.  Less Noise.  Initial Convolution before the first residual block.\n",
    "Total params: 32,573\n",
    "Trainable params: 32,573.  Still a tiny number of params.\n",
    "```\n",
    "\tepoch\tacc\tdice_coefficient\tloss\tval_acc\tval_dice_coefficient\tval_loss\n",
    "0\t0\t0.086059\t0.009101\t-0.009101\t0.003821\t0.000005\t-0.000005\n",
    "10\t10\t0.024164\t0.037224\t-0.037224\t0.000000\t0.000004\t-0.000004\n",
    "20\t20\t0.011664\t0.019532\t-0.019532\t0.000383\t0.000768\t-0.000768\n",
    "25\t25\t0.012666\t0.021993\t-0.021993\t0.002054\t0.004086\t-0.004086\n",
    "```\n",
    "\n",
    "n_r = 16, n_a = 4.  Noise=0.001. Terrible.  Nothing in the masks.\n",
    "\n",
    "```\n",
    "epoch\tacc\tdice_coefficient\tloss\tval_acc\tval_dice_coefficient\tval_loss\n",
    "0\t0\t0.160594\t0.030665\t-0.030665\t0.231094\t0.041545\t-0.041545\n",
    "10\t10\t0.001721\t0.002200\t-0.002200\t0.000032\t0.000006\t-0.000006\n",
    "19\t19\t0.022372\t0.035785\t-0.035785\t0.004810\t0.009487\t-0.009487\n",
    "\n",
    "Epoch 20:\n",
    "Confusion matrix, without normalization\n",
    "[[786432]]\n",
    "```\n",
    "\n",
    "n_r = 16, n_a = 4.  Nice training dice_coefficient but terrible results. Nothing in the masks.\n",
    "```\n",
    "epoch\tacc\tdice_coefficient\tloss\tval_acc\tval_dice_coefficient\tval_loss\n",
    "0\t0\t0.824645\t0.042474\t-0.042474\t0.896317\t0.379815\t-0.379815\n",
    "10\t10\t0.987212\t0.850007\t-0.850007\t1.000000\t1.000000\t-1.000000\n",
    "19\t19\t0.974384\t0.652024\t-0.652024\t1.000000\t1.000000\t-1.000000\n",
    "```\n",
    "\n",
    "n_r = 4, n_a = 4\n",
    "```\n",
    "   epoch       acc  dice_coefficient       loss   val_acc  \\\n",
    "0      0  0.934910          0.000342  14.368396  0.919477   \n",
    "9      9  0.337563          0.044373   2.465352  0.569238   \n",
    "\n",
    "   val_dice_coefficient   val_loss  \n",
    "0              0.002778  56.042610  \n",
    "9              0.000008   0.711905  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,1.0]) # Show results on 0..1 range\n",
    "plt.plot(log_data[\"acc\"])\n",
    "plt.plot(log_data[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(log_data[\"loss\"])\n",
    "plt.plot(log_data[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Dice Coefficient\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.legend(['Training Dice Coefficient', \"Validation Dice Coefficient\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Results Over Time\n",
    "\n",
    "Visualize how the results of the model improve over time.\n",
    "\n",
    "TODO: Why do the confusion matrices look broken for epoch 10 and 20?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelutil.confusion_matrix_by_epochs(\n",
    "    MODELS_DIR, MODEL_NAME, [1, 10, 20], train_gen, \n",
    "    custom_objects={'dice_coefficient_loss': dice_coefficient_loss,\n",
    "                    'dice_coefficient': metrics.dice_coefficient,\n",
    "                    'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Masks by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [1, 10, 20]\n",
    "epochs = [20]\n",
    "train_gen.config(batch_size=1, length=10, num_samples=1, crop_shape=None, flip=None, transpose=None, gray_std=None)\n",
    "for epoch in epochs:\n",
    "    print('Epoch', epoch)\n",
    "    model = modelutil.get_epoch_model(MODELS_DIR, MODEL_NAME, epoch,\n",
    "                                      custom_objects={'dice_coefficient_loss': dice_coefficient_loss, \n",
    "                                                      'dice_coefficient': metrics.dice_coefficient,\n",
    "                                                      'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    for i in range(len(train_gen)):\n",
    "        print('Sequence', i)\n",
    "        x, y = train_gen[i]\n",
    "        print(x.shape)\n",
    "        for j in range(x.shape[0]): # batch size\n",
    "            print('Input')\n",
    "            display(util.animate_crop(x[j, :, :, :, 0], step=20))\n",
    "            print('True')\n",
    "            display(util.animate_crop(y[j, :, :, :, 0], step=20))\n",
    "            print('predicting...')\n",
    "            y_pred = model.predict_on_batch(x)\n",
    "            y_pred = y_pred > BINARY_MASK_THRESH\n",
    "            print('Predicted')\n",
    "            display(util.animate_crop(y_pred[j, :, :, :, 0], step=20))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
