{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "Code to create data generators using preprocessed nifti data from UVMMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, Conv3D, \n",
    "                          UpSampling3D, BatchNormalization)\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import util\n",
    "importlib.reload(util)\n",
    "import preprocessing\n",
    "importlib.reload(preprocessing)\n",
    "\n",
    "SEED = 0\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "PATCH_SHAPE = (32, 32, 32)\n",
    "\n",
    "MODEL_NAME = 'model_01'\n",
    "\n",
    "DATA_DIR = Path('/data2').expanduser()\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_LOG_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_LOG_DIR, TMP_DIR, PP_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators\n",
    "\n",
    "Data generators yield batch-sized random samples of training and validation data.  We used the keras analogue, a keras.utils.Sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, shape):\n",
    "    '''\n",
    "    Randomly crop an image to a shape.  Location is chosen at random from\n",
    "    all possible crops of the given shape.\n",
    "    \n",
    "    img: a volume to crop\n",
    "    shape: size of cropped volume.  e.g. (32, 32, 32)\n",
    "    '''\n",
    "    assert all(img.shape[i] >= shape[i] for i in range(len(shape)))\n",
    "    \n",
    "    # if img.shape[i] == 32 and shape[i] == 32, i_max == 0.\n",
    "    maxes = [img.shape[i] - shape[i] for i in range(len(shape))]\n",
    "    # the starting corner of the crop\n",
    "    starts = [random.randint(0, m) for m in maxes]\n",
    "    # Will this indexing work?\n",
    "    cropped_img = img[[slice(starts[i], starts[i] + shape[i]) for i in range(len(shape))]]\n",
    "    return cropped_img\n",
    "        \n",
    "\n",
    "def augment_image(img, crop_shape):\n",
    "    return random_crop(img, crop_shape)\n",
    "\n",
    "\n",
    "class ScanSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_infos, batch_size, crop_shape, shuffle=True):\n",
    "        '''\n",
    "        x_paths: list of paths to preprocessed images\n",
    "        '''\n",
    "        self.x = x_infos.reset_index()\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.crop_shape = crop_shape\n",
    "        # assert len(self.x) == len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return number of batches, based on batch_size\n",
    "        '''\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx: batch index\n",
    "        '''\n",
    "        # loc indexing uses inclusive name-based indexing, I know I know don't ask, hence the -1.\n",
    "        batch_x_paths = list(self.x.loc[idx * self.batch_size:(idx + 1) * self.batch_size - 1, 'pp_path'])\n",
    "        # add channel dimension to each augmented (randomly cropped) image.\n",
    "        batch_x = [np.expand_dims(augment_image(preprocessing.get_preprocessed_image(path), \n",
    "                                                crop_shape=self.crop_shape), axis=-1)\n",
    "                   for path in batch_x_paths]\n",
    "\n",
    "        # return x and y batches\n",
    "        return (np.array(batch_x), np.array(batch_x))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            self.x = self.x.sample(frac=1) # shuffle x\n",
    "    \n",
    "\n",
    "def get_datagens(preprocessed_metadata_path, batch_size, crop_shape, seed=0, validation_split=0.25):\n",
    "    '''\n",
    "    Return a tuple of training ScanSequence and validation ScanSequence\n",
    "    '''\n",
    "    # Data generator\n",
    "    infos = preprocessing.read_preprocessed_metadata(preprocessed_metadata_path)\n",
    "    print('Data set size:', len(infos))\n",
    "    shuffled = infos.sample(frac=1, random_state=seed)\n",
    "    nrow = len(shuffled)\n",
    "    idx = int(nrow * validation_split)\n",
    "    val = shuffled.iloc[:idx, :].reindex()\n",
    "    train = shuffled.iloc[idx:, :].reindex()\n",
    "    print('Validation set size:', len(val))\n",
    "    print('Train set size:', len(train))\n",
    "    train_gen = ScanSequence(train, batch_size, crop_shape)\n",
    "    val_gen = ScanSequence(val, batch_size, crop_shape)\n",
    "    return train_gen, val_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Validating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the random crop is producing what look like random crops.\n",
    "img = preprocessing.get_preprocessed_image(preprocessing.read_preprocessed_metadata(PP_MD_PATH).loc[0, 'pp_path'])\n",
    "display(animate_crop(img, step=1))\n",
    "for i in range(5):\n",
    "    display(animate_crop(random_crop(img, PATCH_SHAPE), step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test getting a batch of data from ScanSequence\n",
    "seq, _ = get_datagens(preprocessed_metadata_path=PP_MD_PATH, batch_size=BATCH_SIZE, crop_shape=PATCH_SHAPE)\n",
    "print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that a batch picture looks like a preprocessed image.\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "display(animate_crop(batch_x[0, :, :, :, 0])) # drop the example and channel dimensions\n",
    "display(animate_crop(batch_y[0, :, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine preprocessed metadata for any weirdness\n",
    "\n",
    "Found one scan, for id 082222_190, with a bogus shape (only one slice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = preprocessing.read_preprocessed_metadata(PP_MD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos[['pp_dim0', 'pp_dim1', 'pp_dim2']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos[infos['pp_dim0'] == 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = infos[infos['id'] != '082222_190']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos[['pp_dim0', 'pp_dim1', 'pp_dim2']].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos[(infos['pp_dim0'] == 234) | (infos['pp_dim1'] == 280) | (infos['pp_dim2'] == 278)]\n",
    "# infos[infos['pp_dim1'] == 280]\n",
    "# infos[infos['pp_dim0'] == 278]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
