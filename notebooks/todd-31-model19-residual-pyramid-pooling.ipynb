{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 19 Residual Pyramid Pooling\n",
    "\n",
    "So between straight-net-depth and straight-net-channels, we see a relationship between OOM errors and going beyond a full network volume related to (128 * 128 * 128) volume * 16 channels * 32 residual blocks * 2 layers (conv and sum) layers per block.\n",
    "\n",
    "Experiment with pyramid pooling.\n",
    "\n",
    "U-net uses only around 30 layers.  \n",
    "V-net uses only around 40 layers + residual connections all over the place.\n",
    "\n",
    "GPU: Tesla K80 11GB.\n",
    "\n",
    "### Results\n",
    "\n",
    "- training memory\n",
    "- fov\n",
    "- training speed\n",
    "\n",
    "\n",
    "### To Try\n",
    "\n",
    "\n",
    "- u-net or v-net\n",
    "- spatial pyramid pooling\n",
    "- Use small patches, small batches, batchnorm\n",
    "- making sure my loss functions work.\n",
    "- mean squared error loss\n",
    "- Add dilated convolution stack to end of network (small fov increase).\n",
    "- Using Dropout (try 0.1)\n",
    "- A shallow u-net: Pooling once and taking advantage of the smaller volume to increase channels and layers.  This would lead to a greatly increased fov.  \n",
    "- experiment with downsampling: try stride 2 2x2x2 conv like in v-net, not that they offered much justification for why this was better than the usual stride 2 3x3x3 conv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D,\n",
    "                          Conv3D, UpSampling3D, BatchNormalization, Concatenate, Add,\n",
    "                          GaussianNoise, Dropout\n",
    "                         )\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import SimpleITK # xvertseg MetaImage files\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import util\n",
    "import preprocessing\n",
    "import datagen\n",
    "import modelutil\n",
    "import xvertseg\n",
    "import augmentation\n",
    "import metrics\n",
    "\n",
    "MODEL_NAME = 'model_19'\n",
    "\n",
    "DATA_DIR = Path('/data2').expanduser()\n",
    "# DATA_DIR = Path('~/data/2018').expanduser()\n",
    "# UVMMC\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "# xVertSeg\n",
    "XVERTSEG_DIR = DATA_DIR / 'xVertSeg.v1'\n",
    "PP_XVERTSEG_DIR = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed' # preprocessed scans dir\n",
    "PP_XVERTSEG_MD_PATH = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_DIR, TMP_DIR, PP_MD_PATH.parent, PP_XVERTSEG_DIR, PP_XVERTSEG_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# I love u autoreload!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25 # random seed for dataset shuffling and splitting.\n",
    "VALIDATION_SPLIT = 0.2 # 3 samples for validation\n",
    "TEST_SPLIT = 0.134 # 2 samples for test\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_BATCHES = 10 # The number of batches per epoch or None\n",
    "NUM_SAMPLES = 1 # Show each image num_samples times per epoch. Ignored if N_BATCHES is set.\n",
    "MAX_QUEUE_SIZE = 20\n",
    "EPOCHS = 30\n",
    "\n",
    "# PATCH_SHAPE = (32, 32, 32)\n",
    "# PATCH_SHAPE = (64, 64, 64) # Used to crop images for training (data augmentation, memory, speed)\n",
    "PATCH_SHAPE = (128, 128, 128) # Big.  Good for visualization.\n",
    "# PATCH_SHAPE = None # Full sized images\n",
    "\n",
    "# INPUT_SHAPE = (PATCH_SHAPE + (1,)) # Model input shape adds channel dimension, but not examples dim.\n",
    "INPUT_SHAPE = (None, None, None, 1) # Accept variable size volumes/images.\n",
    "\n",
    "BINARY_MASK_THRESH = 0.5 # > threshold = 1. <= thresh = 0.\n",
    "\n",
    "TRANSPOSE = False\n",
    "FLIP = 0.5\n",
    "GRAY_STD = 0.01\n",
    "\n",
    "# Visualize model using the first set of hyperparams\n",
    "# KERNEL_SIZE = (7, 7, 7)\n",
    "# KERNEL_SIZE = (5, 5, 5)\n",
    "KERNEL_SIZE = (3, 3, 3)\n",
    "N_A = 16 # number of channels\n",
    "N_B = 4 # number of pooling blocks\n",
    "N_R = 3 # number of residual blocks in the pooling blocks\n",
    "N_D = 3 # Depth of pyramid pooling. 4 means downsampling by 2, 4, 8 and 16\n",
    "DROPOUT = None # 0.1\n",
    "NOISE = 0.0001\n",
    "\n",
    "W0 = 1 # binary cross entropy weight for class 0\n",
    "W1 = 100 # weight informed by the 1-to-0 ratio in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_func = lambda: xvertseg.read_xvertseg_metadata(PP_XVERTSEG_MD_PATH)\n",
    "train_gen, val_gen, test_gen = xvertseg.get_xvertseg_datagens(\n",
    "    infos_func, seed=SEED, validation_split=VALIDATION_SPLIT, test_split=TEST_SPLIT)\n",
    "\n",
    "train_gen.config(batch_size=BATCH_SIZE, length=N_BATCHES, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "                 transpose=TRANSPOSE, gray_std=GRAY_STD, num_samples=NUM_SAMPLES).reindex()\n",
    "val_gen.config(batch_size=BATCH_SIZE, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "               transpose=TRANSPOSE, gray_std=GRAY_STD).reindex()\n",
    "# val_gen.config(batch_size=1).reindex() # Test full image\n",
    "test_gen.config(batch_size=1).reindex() # Evaluate using full image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed channel reduction to get around residual pooling.  Altnative could be to\n",
    "# pool, then convolve once to n_c, then to residual convovle.\n",
    "# removed concatenate/convovle in favor of sum\n",
    "def residual_pyramid_pooling_block(x, n_d=1, n_r=1, kernel_size=(3, 3, 3)):\n",
    "    '''\n",
    "    Halves volume size d times by average pooling. \n",
    "    Avg pooling suggested by Pyramid Scene Parsing Network paper, I think.\n",
    "    n_d: depth of pyramid.  This many poolings will be done, each half the size of \n",
    "      the previous one.  The pyramid pooling paper might have done this somewhat\n",
    "      differently.\n",
    "    n_r: number of residual convolutions to perform after pooling and before upsampling.    \n",
    "    '''\n",
    "    if n_d == 0:\n",
    "        return x\n",
    "\n",
    "    x_init = x\n",
    "    n_c = int(x.shape[-1]) # assuming channels last\n",
    "    \n",
    "    xds = [] # one for each depth\n",
    "    for i in range(n_d):\n",
    "        pool_size = (2**(i+1),) * 3 # e.g. (2, 2, 2), (4, 4, 4), (8, 8, 8)\n",
    "        xd = AveragePooling3D(pool_size=pool_size)(x) # n_c channels\n",
    "        for i2 in range(n_r):\n",
    "            xd_init = xd\n",
    "            xd = Conv3D(n_c, kernel_size=kernel_size, padding='same', activation='relu')(xd)\n",
    "            xd = Add()([xd_init, xd])\n",
    "            \n",
    "        xd = UpSampling3D(size=pool_size)(xd)\n",
    "        xds.append(xd)\n",
    "        \n",
    "    x = Add()(xds + [x_init])\n",
    "    return x\n",
    "    \n",
    "\n",
    "def build_model(input_shape, n_a=4, n_d=0, n_b=4, n_r=4, \n",
    "                noise=None, loss='binary_crossentropy', metrics=[],\n",
    "                kernel_size=(3, 3, 3)):\n",
    "    '''\n",
    "    '''\n",
    "\n",
    "    x_input = Input(shape=input_shape)\n",
    "    x = x_input\n",
    "    \n",
    "    if noise: \n",
    "        x = GaussianNoise(stddev=noise)(x)\n",
    "\n",
    "    x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "    \n",
    "    for i in range(n_b):\n",
    "        x = residual_pyramid_pooling_block(x, n_d=n_d, n_r=n_r, kernel_size=kernel_size)\n",
    "\n",
    "    y = Conv3D(1, kernel_size=(1, 1, 1), activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'] + metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_binary_crossentropy_loss = metrics.weighted_binary_crossentropy_loss_func(w0=W0, w1=W1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape=INPUT_SHAPE, n_a=N_A, n_b=N_B, n_r=N_R, n_d=N_D, \n",
    "                    noise=NOISE, \n",
    "#                     loss='binary_crossentropy',\n",
    "#                     loss=metrics.dice_coefficient_loss,\n",
    "                    loss=metrics.dice_coefficient2_loss,\n",
    "#                     loss=weighted_binary_crossentropy_loss,\n",
    "                    metrics=[metrics.dice_coefficient, metrics.binary_dice_coefficient],\n",
    "                    kernel_size=KERNEL_SIZE)\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [modelutil.get_tensorboard_callback(TENSORBOARD_DIR, MODEL_NAME),\n",
    "             modelutil.get_logger_callback(LOG_DIR, MODEL_NAME),\n",
    "             modelutil.get_checkpoint_callback(MODELS_DIR, MODEL_NAME),\n",
    "            ]\n",
    "# datagen shuffles every epoch\n",
    "history = model.fit_generator(train_gen, epochs=EPOCHS, validation_data=val_gen, \n",
    "                              callbacks=callbacks, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                              use_multiprocessing=False, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Notes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "K=(3, 3, 3), N_A=16, N_B=4, N_R=3, N_D=3\n",
    "repeated residual pyramid pooling\n",
    "20 sec/epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metrics from the log file\n",
    "# get latest log path\n",
    "log_path = sorted(LOG_DIR.glob(f'{MODEL_NAME}*_log.csv'))[-1]\n",
    "print(log_path)\n",
    "log_data = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([log_data[::10], log_data[-1:]]) # every 10th metric and the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,1.0]) # Show results on 0..1 range\n",
    "plt.plot(log_data[\"acc\"])\n",
    "plt.plot(log_data[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(log_data[\"loss\"])\n",
    "plt.plot(log_data[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Dice Coefficient\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.legend(['Training Dice Coefficient', \"Validation Dice Coefficient\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Results Over Time\n",
    "\n",
    "Visualize how the results of the model improve over time.\n",
    "\n",
    "TODO: Why do the confusion matrices look broken for epoch 10 and 20?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [1, 10, 20]\n",
    "epochs = [20]\n",
    "for epoch in epochs:\n",
    "    print('Epoch', epoch)\n",
    "    model = modelutil.get_epoch_model(MODELS_DIR, MODEL_NAME, epoch,\n",
    "                                      custom_objects={\n",
    "                                          'dice_coefficient_loss': metrics.dice_coefficient_loss, \n",
    "                                          'dice_coefficient2_loss': metrics.dice_coefficient2_loss,\n",
    "#          'weighted_binary_crossentropy_loss': weighted_binary_crossentropy_loss,\n",
    "                                                      'dice_coefficient': metrics.dice_coefficient,\n",
    "                                                      'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    modelutil.plot_binary_confusion_matrix(model, train_gen)\n",
    "# modelutil.confusion_matrix_by_epochs(\n",
    "#     MODELS_DIR, MODEL_NAME, [1, 10, 20], train_gen, \n",
    "#     custom_objects={\n",
    "#         'dice_coefficient_loss': metrics.dice_coefficient_loss,\n",
    "#         'dice_coefficient2_loss': metrics.dice_coefficient2_loss,\n",
    "# #         'weighted_binary_crossentropy_loss': weighted_binary_crossentropy_loss,\n",
    "#                     'dice_coefficient': metrics.dice_coefficient,\n",
    "        \n",
    "#                     'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Masks by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [1, 10, 20]\n",
    "epochs = [20]\n",
    "train_gen.config(batch_size=1, length=10, num_samples=1, crop_shape=None, flip=None, transpose=None, gray_std=None)\n",
    "for epoch in epochs:\n",
    "    print('Epoch', epoch)\n",
    "    model = modelutil.get_epoch_model(MODELS_DIR, MODEL_NAME, epoch,\n",
    "                                      custom_objects={\n",
    "                                          'dice_coefficient_loss': metrics.dice_coefficient_loss, \n",
    "                                          'dice_coefficient2_loss': metrics.dice_coefficient2_loss,\n",
    "#          'weighted_binary_crossentropy_loss': weighted_binary_crossentropy_loss,\n",
    "                                                      'dice_coefficient': metrics.dice_coefficient,\n",
    "                                                      'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    for i in range(len(train_gen)):\n",
    "        print('Sequence', i)\n",
    "        x, y = train_gen[i]\n",
    "        print(x.shape)\n",
    "        for j in range(x.shape[0]): # batch size\n",
    "            print('Input')\n",
    "            display(util.animate_crop(x[j, :, :, :, 0], step=20))\n",
    "            print('True')\n",
    "            display(util.animate_crop(y[j, :, :, :, 0], step=20))\n",
    "            print('predicting...')\n",
    "            y_pred = model.predict_on_batch(x)\n",
    "            y_pred = y_pred > BINARY_MASK_THRESH\n",
    "            print('Predicted')\n",
    "            display(util.animate_crop(y_pred[j, :, :, :, 0], step=20))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
