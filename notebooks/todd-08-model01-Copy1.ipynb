{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 01\n",
    "\n",
    "First model.  Do something simple with segmentation or autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, Conv3D, \n",
    "                          UpSampling3D, BatchNormalization)\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "\n",
    "SEED = 0\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "PATCH_SHAPE = (32, 32, 32)\n",
    "\n",
    "MODEL_NAME = 'model_01'\n",
    "\n",
    "DATA_DIR = Path('~/data/2018').expanduser()\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_LOG_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_LOG_DIR, TMP_DIR, PP_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def temp_gif_path(tmp_dir=TMP_DIR):\n",
    "    '''\n",
    "    Used to junk up the filesystem with temporary files for animated gifs of ct scans.\n",
    "    '''\n",
    "    return str(tmp_dir / ('tmp_' + uuid.uuid4().hex + '.gif'))\n",
    "\n",
    "    \n",
    "def get_nifti_files(path):\n",
    "    '''\n",
    "    Return a list of Path objs for every .nii file within path.\n",
    "    '''\n",
    "    return list(path.glob('**/*.nii'))\n",
    "\n",
    "\n",
    "def sample_stack(stack, rows=3, cols=3, start_with=0, show_every=3, r=0):\n",
    "    '''\n",
    "    Plot a grid of images (2d slices) sampled from stack.\n",
    "    \n",
    "    stack: 3-d voxel array.\n",
    "    '''\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=[20, 20])\n",
    "    for i in range(rows * cols):\n",
    "        ind = start_with + i * show_every\n",
    "        ax[i // cols, i % cols].set_title('slice %d' % ind)\n",
    "        \n",
    "        if r == 0:\n",
    "            ax[i // cols, i % cols].imshow(stack[:, :, ind], cmap='gray')\n",
    "        else:\n",
    "            ax[i // cols, i % cols].imshow(rotate(stack[:, :, ind], r), cmap='gray')\n",
    "        \n",
    "        \n",
    "        ax[i // cols, i % cols].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_animated_gif(path, img, start=0, stop=None, step=1):\n",
    "    '''\n",
    "    Create animated gif of 3d image, where each frame is a 2-d image taken from \n",
    "    iterating across the 3rd dimension.  E.g. the ith 2d image is img[:, :, i]\n",
    "    path: where to save the animated gif\n",
    "    img: a 3-d volume\n",
    "    start: index of 3rd dimension to start iterating at.  default = 0.\n",
    "    stop: index of 3rd dimension to stop at, not inclusive.  Default is None, meaning stop at img.shape[2].\n",
    "    step: number of slices to skip    \n",
    "    '''\n",
    "    # convert to uint8 to suppress warnings from imageio\n",
    "    imax = img.max()\n",
    "    imin = img.min()\n",
    "    img = 255 * ((img - imin) / (imax - imin)) # scale to 0..255\n",
    "    img = np.uint8(img)\n",
    "    \n",
    "    with imageio.get_writer(path, mode='I') as writer:\n",
    "        for i in range(start, img.shape[2], step):\n",
    "            writer.append_data(img[:, :, i])\n",
    "\n",
    "    \n",
    "def animate_crop(img, crop=(0, 1, 0, 1, 0, 1), axis=2, step=5):\n",
    "    '''\n",
    "    img: a 3d volume to be cropped and animated.\n",
    "    axis: 0, 1, 2: the axis to animate along.  img will be transposed s.t. this axis is the 3rd axis.\n",
    "    crop: 6 element list: axis 0 start position, axis 0 end position, axis 1 start position, etc.  Each position \n",
    "      is a number in [0.0, 1.0] representing the position as a proportion of that axis.  0.0 is the beginning,\n",
    "      1.0 the end, and 0.5 the middle.\n",
    "    step: only include every nth frame in the animation, where each frame is a 2d slice of img.\n",
    "    return: ipython Image, for display in a notebook.\n",
    "    '''\n",
    "    # as a proportion of the total range, range of axis 0, 1, and 2 that should be included in the volume\n",
    "    prop0 = crop[0:2]\n",
    "    prop1 = crop[2:4]\n",
    "    prop2 = crop[4:6]\n",
    "    # as specific voxel coordinates, range of axis 0, 1, and 2 that should be included in the volume\n",
    "    pix0 = [int(p * img.shape[0]) for p in prop0]\n",
    "    pix1 = [int(p * img.shape[1]) for p in prop1]\n",
    "    pix2 = [int(p * img.shape[2]) for p in prop2]\n",
    "\n",
    "    cropped_img = img[pix0[0]:pix0[1], pix1[0]:pix1[1], pix2[0]:pix2[1]]\n",
    "    # rotate axes for animation\n",
    "    cropped_img = cropped_img.transpose([0,1,2][-(2-axis):] + [0,1,2][:-(2-axis)])\n",
    "    \n",
    "    tmp_path = temp_gif_path()\n",
    "    print('temp gif path:', tmp_path)\n",
    "    make_animated_gif(tmp_path, cropped_img, step=step)\n",
    "    return Image(filename=tmp_path)\n",
    "\n",
    "\n",
    "def animate_scan_info_crop(scan_info, i, crop=(0, 1, 0, 1, 0, 1), axis=0, step=3):\n",
    "    path = scan_info.loc[i, 'path']\n",
    "    print('scan path:', path)\n",
    "    img = nib.load(path).get_data()\n",
    "    print('scan img shape:', img.shape)\n",
    "    return animate_crop(img, crop, axis=axis, step=step)\n",
    "    \n",
    "\n",
    "def get_data_infos(paths):\n",
    "    '''\n",
    "    paths: paths to nifti scans.\n",
    "    get file paths, image paths, other useful information about data.\n",
    "    Can be randomly shuffled and split to for train and test set.\n",
    "    Generator will split examples into batch sizes and get associated normalized images and labels.  \n",
    "    '''\n",
    "    infos = pd.DataFrame({'id': [re.sub('\\.nii$', '', p.name) for p in paths], 'path': [str(p) for p in paths]})\n",
    "    infos['nft'] = infos.path.apply(lambda p: nib.load(p))\n",
    "    infos['header'] = infos.nft.apply(lambda nft: nft.header)\n",
    "    infos['affine'] = infos.nft.apply(lambda nft: nft.affine)\n",
    "    infos['pixdim'] = infos.header.apply(lambda h: h['pixdim'][1:4])\n",
    "    infos['dim'] = infos.header.apply(lambda h: h['dim'][1:4])\n",
    "    infos['qform_code'] = infos.header.apply(lambda h: h['qform_code'])\n",
    "    infos['sform_code'] = infos.header.apply(lambda h: h['sform_code'])\n",
    "    infos['sizeof_hdr'] = infos.header.apply(lambda h: h['sizeof_hdr'])\n",
    "    infos['pixdim0'] = infos.pixdim.apply(lambda x: x[0])\n",
    "    infos['pixdim1'] = infos.pixdim.apply(lambda x: x[1])\n",
    "    infos['pixdim2'] = infos.pixdim.apply(lambda x: x[2])\n",
    "    infos['dim0'] = infos.dim.apply(lambda x: x[0])\n",
    "    infos['dim1'] = infos.dim.apply(lambda x: x[1])\n",
    "    infos['dim2'] = infos.dim.apply(lambda x: x[2])\n",
    "    infos['desc'] = infos.header.apply(lambda h: h['descrip'])\n",
    "    infos['class'] = ['normal'] * infos.shape[0]\n",
    "    infos.reset_index()\n",
    "    return infos\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "1. Read in the original images.\n",
    "2. Resample the image s.t the voxel size is 1mm x 1mm x 1mm.\n",
    "3. Clip houndsfield unit values and normalize them to be between 0 and 1. (like Julian de Wit recommends for Kaggle)\n",
    "\n",
    "Some processing code is from https://github.com/juliandewit/kaggle_ndsb2017/blob/master/step1_preprocess_luna16.py.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_image(path):\n",
    "    # read the image from the filesystem\n",
    "    img = nib.load(path).get_data()\n",
    "    return img\n",
    "   \n",
    "    \n",
    "def get_preprocessed_image(path):\n",
    "    return np.load(path)\n",
    "\n",
    "    \n",
    "def get_processed_image(info, crop_shape=PATCH_SHAPE):\n",
    "    img = get_image(info['path'])\n",
    "    scan_id = info['id']\n",
    "\n",
    "    # preprocess image\n",
    "    # use 1mm^3 voxel size to reduce image size.\n",
    "    spacing = (info['pixdim0'], info['pixdim1'], info['pixdim2'])\n",
    "    new_spacing = (1.0, 1.0, 1.0)\n",
    "    resampled_img, adj_new_spacing = resample_image(img, spacing, new_spacing)\n",
    "    \n",
    "    # augment image for training\n",
    "    cropped_img = random_crop(resampled_img, crop_shape)\n",
    "    \n",
    "    \n",
    "    return cropped_img, adj_new_spacing\n",
    "    \n",
    "\n",
    "def resample_image(image, spacing, new_spacing):\n",
    "    '''\n",
    "    image: a 3d volume\n",
    "    spacing: the size of a voxel in some units.  E.g. [0.3, 0.3, 0.9]\n",
    "    new_spacing: the size of a voxel after resampling, in some units.  E.g. [1.0, 1.0, 1.0]\n",
    "    \n",
    "    returns: resampled image and new spacing adjusted because images have integer dimensions.\n",
    "    '''\n",
    "    # calculate resize factor required to change image to new shape\n",
    "    spacing = np.array(spacing)\n",
    "    new_spacing = np.array(new_spacing)\n",
    "    spacing_resize_factor = spacing / new_spacing\n",
    "    new_real_shape = image.shape * spacing_resize_factor\n",
    "    new_shape = np.round(new_real_shape)\n",
    "    real_resize_factor = new_shape / image.shape\n",
    "    \n",
    "    # adjusted spacing to account for integer dimensions of resized image.\n",
    "    new_spacing = spacing / real_resize_factor\n",
    "    \n",
    "    new_image = scipy.ndimage.interpolation.zoom(image, real_resize_factor)\n",
    "    return new_image, new_spacing\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    '''\n",
    "    Normalize voxel units by clipping them to lie between -1000 and 1000 hounsfield units \n",
    "    and then scale number to between 0 and 1.\n",
    "    '''\n",
    "    MIN_BOUND = -1000.0 # Air: -1000, Water: 0 hounsfield units.\n",
    "    MAX_BOUND = 1000.0 # Bone: 200, 700, 3000.  https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "    image = (image - MIN_BOUND) / (MAX_BOUND - MIN_BOUND)\n",
    "    image[image > 1] = 1.\n",
    "    image[image < 0] = 0.\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_preprocessed_image_path(scan_id, preprocessed_dir):\n",
    "    return str(Path(preprocessed_dir, f'{scan_id}.npy'))\n",
    "\n",
    "\n",
    "def preprocess_nifti_normals(src_dir=NORMAL_SCANS_DIR, dest_dir=PP_IMG_DIR, delete_existing=False, \n",
    "                             metadata_path=PP_MD_PATH):\n",
    "    \n",
    "    if delete_existing and dest_dir.isdir():\n",
    "        print('Removing existing dest dir:', dest_dir)\n",
    "        shutil.rmtree(dest_dir)\n",
    "    if not dest_dir.exists():\n",
    "        print('Making preprocessed images destination:', dest_dir)\n",
    "        dest_dir.mkdir(parents=True)\n",
    "        \n",
    "    infos = get_data_infos(get_nifti_files(src_dir))\n",
    "    spacings = {}\n",
    "    preprocessed_paths = {}\n",
    "    for i in range(len(infos)):\n",
    "        info = infos.loc[i, :]\n",
    "        img_path = info['path']\n",
    "        img = get_image(img_path)\n",
    "        scan_id = info['id']\n",
    "        print('image index:', i)\n",
    "        print('image id:', scan_id)\n",
    "        print('image shape:', img.shape)\n",
    "        print('image path:', img_path)\n",
    "        \n",
    "        # Standardize voxel size to 1mm^3 to reduce image size.\n",
    "        spacing = (info['pixdim0'], info['pixdim1'], info['pixdim2'])\n",
    "        target_spacing = (1.0, 1.0, 1.0)\n",
    "        print('image spacing:', spacing)\n",
    "        print('new spacing:', target_spacing)\n",
    "        resampled_img, resampled_spacing = resample_image(img, spacing, target_spacing)\n",
    "        print('resampled image spacing:', resampled_spacing)\n",
    "        print('resampled image shape:', resampled_img.shape)\n",
    "        \n",
    "        normalized_img = normalize_image(resampled_img)\n",
    "        print('Normalized image shape:', normalized_img.shape)\n",
    "        \n",
    "        # save processed image\n",
    "        path = get_preprocessed_image_path(scan_id, dest_dir)\n",
    "        print(f'Saving preprocessed image to {path}.')\n",
    "        np.save(path, normalized_img)\n",
    "        \n",
    "        # track image metadata\n",
    "        infos.loc[i, 'pp_path'] = str(path)\n",
    "        infos.loc[i, 'pp_pixdim0'] = resampled_spacing[0] # pixdim0\n",
    "        infos.loc[i, 'pp_pixdim1'] = resampled_spacing[1] # pixdim1\n",
    "        infos.loc[i, 'pp_pixdim2'] = resampled_spacing[2] # pixdim2\n",
    "\n",
    "    # save metadata\n",
    "    write_preprocessed_metadata(infos)\n",
    "    return infos\n",
    "    \n",
    "        \n",
    "def write_preprocessed_metadata(infos, path=PP_MD_PATH):\n",
    "    with open(path, 'wb') as fh:\n",
    "        fh.write(pickle.dumps(infos))\n",
    "    \n",
    "    \n",
    "def read_preprocessed_metadata(path=PP_MD_PATH):\n",
    "    with open(path, 'rb') as fh:\n",
    "        infos = pickle.loads(fh.read())\n",
    "    \n",
    "    return infos\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Images and Save to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to preprocess images\n",
    "# infos = preprocess_nifti_normals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Validating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test getting a raw image\n",
    "data_infos = infos\n",
    "img_info = data_infos.iloc[0]\n",
    "img = get_image(img_info['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_crop(img, axis=2, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the resampled image has more or less the shape we expect it to have after resizing the voxels.\n",
    "\n",
    "img_spacing = (img_info['pixdim0'], img_info['pixdim1'], img_info['pixdim2'])\n",
    "print('Shape and spacing before resampleing\\t', img.shape, img_spacing)\n",
    "target_img_spacing = (1., 1., 1.)\n",
    "print('Target spacing:', target_img_spacing)\n",
    "resampled_img, resampled_spacing = resample_image(img, img_spacing, target_img_spacing)\n",
    "print (\"Shape after resampling\\t\", resampled_img.shape, resampled_spacing)\n",
    "animate_crop(resampled_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading metadata, which contains the preprocessed image spacings and file paths\n",
    "infos = read_preprocessed_metadata()\n",
    "pp_spacings = list(zip(infos['pp_pixdim0'], infos['pp_pixdim1'], infos['pp_pixdim2']))\n",
    "pp_paths = list(infos['pp_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_spacings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that preprocessed images look reasonable when visualized\n",
    "for i in range(3):\n",
    "    img = get_preprocessed_image(infos.loc[i, 'pp_path'])\n",
    "    scan_id = infos.loc[i, 'id']\n",
    "    print(f'image {i} scan id {scan_id} shape {img.shape}')\n",
    "    display(animate_crop(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators\n",
    "\n",
    "Yield batch-sized random samples of training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_crop(img, shape):\n",
    "    '''\n",
    "    Randomly crop an image to a shape.  Location is chosen at random from\n",
    "    all possible crops of the given shape.\n",
    "    \n",
    "    img: a volume to crop\n",
    "    shape: size of cropped volume.  e.g. (32, 32, 32)\n",
    "    '''\n",
    "    assert all(img.shape[i] >= shape[i] for i in range(len(shape)))\n",
    "    \n",
    "    # if img.shape[i] == 32 and shape[i] == 32, i_max == 0.\n",
    "    maxes = [img.shape[i] - shape[i] for i in range(len(shape))]\n",
    "    # the starting corner of the crop\n",
    "    starts = [random.randint(0, m) for m in maxes]\n",
    "    # Will this indexing work?\n",
    "    cropped_img = img[[slice(starts[i], starts[i] + shape[i]) for i in range(len(shape))]]\n",
    "    cropped_img = img[starts[0]:(starts[0] + shape[0]),\n",
    "                      starts[1]:(starts[1] + shape[1]),\n",
    "                      starts[2]:(starts[2] + shape[2])]\n",
    "    return cropped_img\n",
    "        \n",
    "\n",
    "def augment_image(img, crop_shape=PATCH_SHAPE):\n",
    "    return random_crop(img, crop_shape)\n",
    "\n",
    "\n",
    "class ScanSequence(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, x_infos, batch_size):\n",
    "        '''\n",
    "        x_paths: list of paths to preprocessed images\n",
    "        '''\n",
    "        self.x = x_infos.reset_index()\n",
    "        self.batch_size = batch_size\n",
    "        # assert len(self.x) == len(self.y)\n",
    "\n",
    "    def __len__(self):\n",
    "        '''\n",
    "        Return number of batches, based on batch_size\n",
    "        '''\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        idx: batch index\n",
    "        '''\n",
    "        # loc indexing uses inclusive name-based indexing\n",
    "        batch_x_paths = list(self.x.loc[idx * self.batch_size:(idx + 1) * self.batch_size - 1, 'pp_path'])\n",
    "        # add channel dimension to each image.\n",
    "        batch_x = [np.expand_dims(get_preprocessed_image(path), axis=-1) for path in batch_x_paths]\n",
    "        \n",
    "        # batch_y = self.y.iloc[idx * self.batch_size:(idx + 1) * self.batch_size, :]\n",
    "\n",
    "        return (np.array(batch_x), np.array(batch_x))\n",
    "    \n",
    "\n",
    "def get_datagens(seed=SEED, validation_split=0.25, preprocessed_metadata_path=PP_MD_PATH, batch_size=BATCH_SIZE):\n",
    "    # Data generator\n",
    "    infos = read_preprocessed_metadata(preprocessed_metadata_path)\n",
    "    shuffled = infos.sample(frac=1, random_state=seed)\n",
    "    nrow = len(shuffled)\n",
    "    idx = int(nrow * validation_split)\n",
    "    val = shuffled.iloc[:idx, :].reindex()\n",
    "    train = shuffled.iloc[idx:, :].reindex()\n",
    "    train_gen = ScanSequence(train, batch_size)\n",
    "    val_gen = ScanSequence(val, batch_size)\n",
    "    return train_gen, val_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and Validating Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that the random crop is producing what look like random crops.\n",
    "img = get_preprocessed_image(pp_paths[0])\n",
    "display(animate_crop(img, step=1))\n",
    "for i in range(5):\n",
    "    display(animate_crop(random_crop(img, PATCH_SHAPE), step=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test getting a batch of data from ScanSequence\n",
    "seq, _ = get_datagens()\n",
    "print(len(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x, batch_y = seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that a batch picture looks like a preprocessed image.\n",
    "print(batch_x.shape, batch_y.shape)\n",
    "display(animate_crop(batch_x[0, :, :, :, 0])) # drop the example and channel dimensions\n",
    "display(animate_crop(batch_y[0, :, :, :, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    '''\n",
    "    n_x: number of input features.  The size of the vocabulary.  Each char is one-hot encoded\n",
    "    n_y: number of output features.  The same as n_x for next character prediction.\n",
    "    n_a: number of hidden units in rnn layer\n",
    "    n_a2: number of hidden units in conv layer\n",
    "    n_t: the length of each sequence.\n",
    "    '''\n",
    "    n_a = 32\n",
    "    ## the input is a sequence of characters that have been one-hot encoded.\n",
    "    x_input = Input(shape=(None, None, None, 1)) # arbitrary shape, 1 channel\n",
    "    x = x_input\n",
    "    x = Conv3D(n_a, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = AveragePooling3D()(x) # shape / 2, n_a filters\n",
    "\n",
    "#     x = Conv3D(n_a * 2, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = AveragePooling3D()(x) # shape / 4, n_a * 2 filters\n",
    "    \n",
    "#     x = Conv3D(n_a * 4, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = AveragePooling3D()(x) # shape / 8, n_a * 4 filters\n",
    "\n",
    "#     x = Conv3D(n_a * 2, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = UpSampling3D()(x) # shape / 4, n_a * 2 filters\n",
    "\n",
    "#     x = Conv3D(n_a * 2, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = UpSampling3D()(x) # shape / 2, n_a filters\n",
    "\n",
    "    x = Conv3D(n_a, kernel_size=(3, 3, 3), padding='same', activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = UpSampling3D()(x) # shape, 1 channel\n",
    "\n",
    "    y = Conv3D(1, kernel_size=(3, 3, 3), padding='same')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "model = build_model()\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Model\n",
    "\n",
    "- Add callbacks to save model every 20 epochs and to log performance stats every epoch, so we have the results saved somewhere for charting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks include ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "# Save the model\n",
    "def train_model(model, train_gen, val_gen, epochs=EPOCHS, batch_size=BATCH_SIZE, models_dir=MODELS_DIR, model_name=MODEL_NAME, log_dir=LOG_DIR,\n",
    "                tensorboard_log_dir=TENSORBOARD_LOG_DIR):\n",
    "    # Saving model\n",
    "    model_path = models_dir  /  (model_name +'_{epoch:02d}.h5')\n",
    "    print('model path:', model_path)\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        str(model_path), monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, \n",
    "        mode='auto', period=1)\n",
    "    \n",
    "    # Stop when validation loss stops improving\n",
    "    early_cb = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "    \n",
    "    # Save logs for each run to logfile\n",
    "    log_path = log_dir / (model_name + '_' + datetime.datetime.now().isoformat() + '_log.csv')\n",
    "    print('log path:', log_path)\n",
    "    log_cb = keras.callbacks.CSVLogger(str(log_path), separator=',', append=False)\n",
    "    \n",
    "    # Enable Tensorboard\n",
    "    print('tensorboard log dir:', tensorboard_log_dir)\n",
    "    tensorboard_cb = keras.callbacks.TensorBoard(log_dir=str(tensorboard_log_dir), \n",
    "                                                 histogram_freq=0, write_graph=True, write_images=True)\n",
    "    \n",
    "    # Fit Model\n",
    "    history = model.fit_generator(train_gen, epochs=epochs, validation_data=val_gen, \n",
    "                        callbacks=[checkpoint_cb, log_cb, tensorboard_cb], max_queue_size=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen, val_gen = get_datagens()\n",
    "history = train_model(model, train_gen, val_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metrics from the log file\n",
    "metrics = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.concat([metrics[::10], metrics[-1:]])) # every 10th metric and the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,1.0]) # Show results on 0..1 range\n",
    "plt.plot(metrics[\"acc\"])\n",
    "plt.plot(metrics[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(metrics[\"loss\"])\n",
    "plt.plot(metrics[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Effect of Training on Text Generation\n",
    "\n",
    "Use models from different training epochs to generate text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_path(model_name, epoch):\n",
    "    model_path = models_dir  /  (model_name + f'_{epoch:02d}.h5')\n",
    "    return model_path\n",
    "\n",
    "\n",
    "def weighted_sample(probs):\n",
    "    '''\n",
    "    probs is a 2d array where each row is a separate probability distribution for the next character\n",
    "    return an index for each row corresponding to a randomly sampled probability.\n",
    "    Example:\n",
    "    [[0.8, 0.1, 0.1],\n",
    "     [0.2, 0.5, 0.3]]\n",
    "    '''\n",
    "    # this has no axis argument\n",
    "    # np.random.choice(len(preds), p=preds)\n",
    "\n",
    "    # https://stackoverflow.com/questions/40474436/how-to-apply-numpy-random-choice-to-a-matrix-of-probability-values-vectorized-s\n",
    "    #cum holds the cumulative distributions:\n",
    "    c = probs.cumsum(axis=1)\n",
    "    # Generate a set of uniformly distributed samples...\n",
    "    u = np.random.rand(len(c), 1)\n",
    "    #...and then see where they \"fit\" in c:\n",
    "    choices = (u < c).argmax(axis=1)\n",
    "    return choices\n",
    "        \n",
    "    \n",
    "def max_sample(probs):\n",
    "    return np.argmax(probs, axis=-1)\n",
    "\n",
    "\n",
    "def seed_text(text, seq_len):\n",
    "    start = np.random.randint(0, len(text) - seq_len)\n",
    "    return text[start:(start + seq_len)]\n",
    "\n",
    "\n",
    "def generate_text_for_epochs(model_name, epochs, text, seq_len, vocab_size, num_samples, sample_len):\n",
    "    for epoch in epochs:\n",
    "        path = get_model_path(model_name, epoch)\n",
    "        model = keras.models.load_model(path)\n",
    "        print('Epoch {}:'.format(epoch))\n",
    "        for i in range(num_samples):\n",
    "            seed, sample = generate_text(model, text, seq_len, vocab_size, sample_len)\n",
    "            print(sample)\n",
    "\n",
    "\n",
    "def generate_text(model, text, seq_len, vocab_size, output_len):\n",
    "    int_to_char = datagen.get_int_to_char(vocab_size)\n",
    "    char_to_int = datagen.get_char_to_int(vocab_size)\n",
    "    # initial sequences to prime the generation of next characters\n",
    "    seed = seed_text(text, seq_len)\n",
    "    # as tensors for input to model.  shape (1, seq_len, vocab_size)\n",
    "    x_seq = seed\n",
    "    output = ''\n",
    "    # generate output_len characters\n",
    "    for i in range(output_len):\n",
    "        x = datagen.sequences_to_tensor([x_seq], seq_len, char_to_int)\n",
    "        preds = model.predict(x)[0] # shape (1, vocab_size)\n",
    "        idx = np.random.choice(len(preds), p=preds)\n",
    "        char = int_to_char[idx]\n",
    "        output += char\n",
    "        x_seq = x_seq[1:] + char\n",
    "    return seed, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_for_epochs(model_name, [20, 40, 60, 80, 100], text, SEQ_LEN, VOCAB_SIZE, NUM_GEN_TEXT_SAMPLES, GEN_SAMPLE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
