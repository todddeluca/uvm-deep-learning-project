{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 17 Straight Net Depth\n",
    "\n",
    "Straight residual convolutions.\n",
    "\n",
    "How many layers deep can the network be before the GPU runs out of memory?\n",
    "\n",
    "### Results\n",
    "\n",
    "A 128 residual convolutions (2 layers each) deep network trained for an epoch, very slowly.  This network, with 3x3x3 convs, has a ~257 fov, possibly more than the guestimated 201 fov of u-net.\n",
    "\n",
    "Depth affects computation and training speed but getting 128 deep (on 128x128x128 crops) is fine in terms of GPU memory. Tesla K80 11GB.\n",
    "\n",
    "\n",
    "### To Try\n",
    "\n",
    "- Use small patches, small batches, batchnorm\n",
    "- making sure my loss functions work.\n",
    "- Try mean squared error loss or weighted binary cross entropy\n",
    "- Add dilated convolution stack to end of network (small fov increase).\n",
    "- Using Dropout (try 0.1)\n",
    "- A shallow u-net: Pooling once and taking advantage of the smaller volume to increase channels and layers.  This would lead to a greatly increased fov.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import keras\n",
    "from keras.layers import (Dense, SimpleRNN, Input, Conv1D, \n",
    "                          LSTM, GRU, AveragePooling3D, MaxPooling3D, GlobalMaxPooling3D,\n",
    "                          Conv3D, UpSampling3D, BatchNormalization, Concatenate, Add,\n",
    "                          GaussianNoise, Dropout\n",
    "                         )\n",
    "from keras.models import Model\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import projd\n",
    "import random\n",
    "import re\n",
    "import scipy\n",
    "import shutil\n",
    "import SimpleITK # xvertseg MetaImage files\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import uuid\n",
    "\n",
    "import matplotlib.pyplot as plt # data viz\n",
    "import seaborn as sns # data viz\n",
    "\n",
    "import imageio # display animated volumes\n",
    "from IPython.display import Image # display animated volumes\n",
    "\n",
    "from IPython.display import SVG # visualize model\n",
    "from keras.utils.vis_utils import model_to_dot # visualize model\n",
    "\n",
    "# for importing local code\n",
    "src_dir = str(Path(projd.cwd_token_dir('notebooks')) / 'src') # $PROJECT_ROOT/src\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "import util\n",
    "import preprocessing\n",
    "import datagen\n",
    "import modelutil\n",
    "import xvertseg\n",
    "import augmentation\n",
    "import metrics\n",
    "\n",
    "MODEL_NAME = 'model_17'\n",
    "\n",
    "DATA_DIR = Path('/data2').expanduser()\n",
    "# DATA_DIR = Path('~/data/2018').expanduser()\n",
    "# UVMMC\n",
    "NORMAL_SCANS_DIR = DATA_DIR / 'uvmmc/nifti_normals'\n",
    "PROJECT_DATA_DIR = DATA_DIR / 'uvm_deep_learning_project'\n",
    "PP_IMG_DIR = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed' # preprocessed scans dir\n",
    "PP_MD_PATH = PROJECT_DATA_DIR / 'uvmmc' / 'preprocessed_metadata.pkl'\n",
    "# xVertSeg\n",
    "XVERTSEG_DIR = DATA_DIR / 'xVertSeg.v1'\n",
    "PP_XVERTSEG_DIR = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed' # preprocessed scans dir\n",
    "PP_XVERTSEG_MD_PATH = PROJECT_DATA_DIR / 'xVertSeg.v1' / 'preprocessed_metadata.pkl'\n",
    "\n",
    "\n",
    "MODELS_DIR = PROJECT_DATA_DIR / 'models'\n",
    "LOG_DIR = PROJECT_DATA_DIR / 'log'\n",
    "TENSORBOARD_DIR = PROJECT_DATA_DIR / 'tensorboard'\n",
    "TMP_DIR = DATA_DIR / 'tmp'\n",
    "\n",
    "for d in [DATA_DIR, NORMAL_SCANS_DIR, PROJECT_DATA_DIR, PP_IMG_DIR, MODELS_DIR, LOG_DIR, \n",
    "          TENSORBOARD_DIR, TMP_DIR, PP_MD_PATH.parent, PP_XVERTSEG_DIR, PP_XVERTSEG_MD_PATH.parent]:\n",
    "    if not d.exists():\n",
    "        d.mkdir(parents=True)\n",
    "        \n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "# I love u autoreload!\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 25 # random seed for dataset shuffling and splitting.\n",
    "VALIDATION_SPLIT = 0.2 # 3 samples for validation\n",
    "TEST_SPLIT = 0.134 # 2 samples for test\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "N_BATCHES = 10 # The number of batches per epoch or None\n",
    "NUM_SAMPLES = 1 # Show each image num_samples times per epoch. Ignored if N_BATCHES is set.\n",
    "MAX_QUEUE_SIZE = 20\n",
    "EPOCHS = 30\n",
    "\n",
    "# PATCH_SHAPE = (32, 32, 32)\n",
    "# PATCH_SHAPE = (64, 64, 64) # Used to crop images for training (data augmentation, memory, speed)\n",
    "PATCH_SHAPE = (128, 128, 128) # Big.  Good for visualization.\n",
    "# PATCH_SHAPE = None # Full sized images\n",
    "\n",
    "# INPUT_SHAPE = (PATCH_SHAPE + (1,)) # Model input shape adds channel dimension, but not examples dim.\n",
    "INPUT_SHAPE = (None, None, None, 1) # Accept variable size volumes/images.\n",
    "\n",
    "BINARY_MASK_THRESH = 0.5 # > threshold = 1. <= thresh = 0.\n",
    "\n",
    "TRANSPOSE = False\n",
    "FLIP = 0.5\n",
    "GRAY_STD = 0.01\n",
    "\n",
    "# Visualize model using the first set of hyperparams\n",
    "# KERNEL_SIZE = (7, 7, 7)\n",
    "# KERNEL_SIZE = (5, 5, 5)\n",
    "KERNEL_SIZE = (3, 3, 3)\n",
    "# n_a = 2, n_r = 8.  NaN loss.  Why?\n",
    "# straight net: (128, 128, 128) patch 4 channels * 32 residual layers too much memory\n",
    "# straight net: (128, 128, 128) patch 4 channels * 16 residual blocks ok (memory wise)\n",
    "# seems near the limit of memory.\n",
    "# straight net: (128, 128, 128) patch 4 channels * 24 residual blocks ok (memory wise)\n",
    "N_A = 4 # number of channels # 4 and 4 works for full sized testing, I think, memory-wise.\n",
    "N_B = 128 + 32 # number of blocks of residual blocks.\n",
    "N_R = 1 # number of repeated layers/blocks.  33 pixel field of view after 8 5x convolutions.\n",
    "DROPOUT = None # 0.1\n",
    "NOISE = None # 0.0001\n",
    "\n",
    "W0 = 1 # binary cross entropy weight for class 0\n",
    "W1 = 100 # weight informed by the 1-to-0 ratio in the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos_func = lambda: xvertseg.read_xvertseg_metadata(PP_XVERTSEG_MD_PATH)\n",
    "train_gen, val_gen, test_gen = xvertseg.get_xvertseg_datagens(\n",
    "    infos_func, seed=SEED, validation_split=VALIDATION_SPLIT, test_split=TEST_SPLIT)\n",
    "\n",
    "train_gen.config(batch_size=BATCH_SIZE, length=N_BATCHES, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "                 transpose=TRANSPOSE, gray_std=GRAY_STD, num_samples=NUM_SAMPLES).reindex()\n",
    "val_gen.config(batch_size=BATCH_SIZE, crop_shape=PATCH_SHAPE, flip=FLIP, \n",
    "               transpose=TRANSPOSE, gray_std=GRAY_STD).reindex()\n",
    "# val_gen.config(batch_size=1).reindex() # Test full image\n",
    "test_gen.config(batch_size=1).reindex() # Evaluate using full image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, n_a=4, n_b=4, n_r=4, dropout=None, noise=None, loss='binary_crossentropy', metrics=[],\n",
    "                kernel_size=3):\n",
    "    '''\n",
    "    3D convolutional straight convolutional segmenter.\n",
    "    \n",
    "    dropout: proportion of activation of input to drop out. 0.0 to 1.0\n",
    "    noise: std dev of noise added to input activation.\n",
    "    w0: > 0.0.  A weight for this class in the binary cross entropy loss.\n",
    "    w1: > 0.0.  A weight for this class in the binary cross entropy loss.\n",
    "\n",
    "    returns: Keras model\n",
    "    '''\n",
    "\n",
    "    x_input = Input(shape=input_shape)\n",
    "    x = x_input\n",
    "    \n",
    "    # noise regularization\n",
    "    if noise: \n",
    "        x = GaussianNoise(stddev=noise)(x)\n",
    "\n",
    "    x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "\n",
    "    # Dropout followed by n_r convolutions.\n",
    "    for i in range(n_b):\n",
    "        if dropout is not None:\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "            \n",
    "        for j in range(n_r):\n",
    "            x_initial = x\n",
    "            # x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "            x = Conv3D(n_a, kernel_size=kernel_size, padding='same', activation='relu')(x)\n",
    "            x = Add()([x_initial, x])  \n",
    "        \n",
    "\n",
    "    y = Conv3D(1, kernel_size=(1, 1, 1), activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=x_input, outputs=y)\n",
    "    model.compile(optimizer='adam', loss=loss, metrics=['accuracy'] + metrics)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted_binary_crossentropy_loss = metrics.weighted_binary_crossentropy_loss_func(w0=W0, w1=W1)\n",
    "dice_coefficient_loss = metrics.dice_coefficient_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(input_shape=INPUT_SHAPE, n_a=N_A, n_b=N_B, n_r=N_R, \n",
    "                    dropout=DROPOUT, noise=NOISE, \n",
    "#                     loss='binary_crossentropy',\n",
    "                    loss=dice_coefficient_loss,\n",
    "                    metrics=[metrics.dice_coefficient, metrics.binary_dice_coefficient],\n",
    "                    kernel_size=KERNEL_SIZE)\n",
    "print(model.summary())\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [modelutil.get_tensorboard_callback(TENSORBOARD_DIR, MODEL_NAME),\n",
    "             modelutil.get_logger_callback(LOG_DIR, MODEL_NAME),\n",
    "             modelutil.get_checkpoint_callback(MODELS_DIR, MODEL_NAME),\n",
    "            ]\n",
    "# datagen shuffles every epoch\n",
    "history = model.fit_generator(train_gen, epochs=EPOCHS, validation_data=val_gen, \n",
    "                              callbacks=callbacks, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                              use_multiprocessing=False, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Notes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "No OOM Error:\n",
    "\n",
    "- (128, 128, 128), (3,3,3), 20 residual convs.  Not fast.\n",
    "- (128, 128, 128), (3,3,3), 24 residual convs.  \n",
    "- (128, 128, 128), (3,3,3), 28 residual convs.  Slow.\n",
    "- (128, 128, 128), (3,3,3), 32 residual convs.  Slow!\n",
    "- (128, 128, 128), (3,3,3), 64 residual convs.  Slow!! \n",
    "  fov 131.\n",
    "- (128, 128, 128), (3,3,3), 128 residual convs.  Slow!!! \n",
    "  10 minute epochs = 1 min / scan.  That's got to be a record.  So few params.\n",
    "  Total params: 55,925\n",
    "  fov 257.  more than u-net's ~201.\n",
    "\n",
    "OOM Error:\n",
    "\n",
    "- (128, 128, 128), (3,3,3), 160 residual convs.  Total params: 69,87\n",
    "  This lines up perfectly with the straight-net-channels notebook, where 16 channels\n",
    "  and 32 depth worked and 32 channels with 16 depth worked, but 32 depth and 20 channels\n",
    "  did not work.  Here 4 * 128 ( = 16 * 32) worked.  And here 4 * 160 ( = 20 * 32) ran OOM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metrics from the log file\n",
    "# get latest log path\n",
    "log_path = sorted(LOG_DIR.glob(f'{MODEL_NAME}*_log.csv'))[-1]\n",
    "print(log_path)\n",
    "log_data = pd.read_csv(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([log_data[::10], log_data[-1:]]) # every 10th metric and the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Training and Validation Accuracy \n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0.0,1.0]) # Show results on 0..1 range\n",
    "plt.plot(log_data[\"acc\"])\n",
    "plt.plot(log_data[\"val_acc\"])\n",
    "plt.legend(['Training Accuracy', \"Validation Accuracy\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.plot(log_data[\"loss\"])\n",
    "plt.plot(log_data[\"val_loss\"])\n",
    "plt.legend(['Training Loss', \"Validation Loss\"])\n",
    "plt.show()\n",
    "\n",
    "# Plot Training and Validation Dice Coefficient\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.plot(log_data[\"dice_coefficient\"])\n",
    "plt.legend(['Training Dice Coefficient', \"Validation Dice Coefficient\"])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Results Over Time\n",
    "\n",
    "Visualize how the results of the model improve over time.\n",
    "\n",
    "TODO: Why do the confusion matrices look broken for epoch 10 and 20?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelutil.confusion_matrix_by_epochs(\n",
    "    MODELS_DIR, MODEL_NAME, [1, 10, 20], train_gen, \n",
    "    custom_objects={'dice_coefficient_loss': dice_coefficient_loss,\n",
    "                    'dice_coefficient': metrics.dice_coefficient,\n",
    "                    'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Masks by Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [1, 10, 20]\n",
    "epochs = [20]\n",
    "train_gen.config(batch_size=1, length=10, num_samples=1, crop_shape=None, flip=None, transpose=None, gray_std=None)\n",
    "for epoch in epochs:\n",
    "    print('Epoch', epoch)\n",
    "    model = modelutil.get_epoch_model(MODELS_DIR, MODEL_NAME, epoch,\n",
    "                                      custom_objects={'dice_coefficient_loss': dice_coefficient_loss, \n",
    "                                                      'dice_coefficient': metrics.dice_coefficient,\n",
    "                                                      'binary_dice_coefficient': metrics.binary_dice_coefficient})\n",
    "    for i in range(len(train_gen)):\n",
    "        print('Sequence', i)\n",
    "        x, y = train_gen[i]\n",
    "        print(x.shape)\n",
    "        for j in range(x.shape[0]): # batch size\n",
    "            print('Input')\n",
    "            display(util.animate_crop(x[j, :, :, :, 0], step=20))\n",
    "            print('True')\n",
    "            display(util.animate_crop(y[j, :, :, :, 0], step=20))\n",
    "            print('predicting...')\n",
    "            y_pred = model.predict_on_batch(x)\n",
    "            y_pred = y_pred > BINARY_MASK_THRESH\n",
    "            print('Predicted')\n",
    "            display(util.animate_crop(y_pred[j, :, :, :, 0], step=20))\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
